# 🏗️ Архитектура решения

## 📋 Задача

Демонстрация работы с ограничениями токенов в YandexGPT-lite:
1. Подсчет токенов (на запрос и ответ)
2. Сравнение короткого, длинного и превышающего лимит запросов
3. Интеллектуальная обрезка/сжатие текста (суммаризация)

## 🎯 Выбранный подход

### Почему гибридная суммаризация?

**Отказались от:**
- ❌ **Простая обрезка** - теряется конец текста, может отрезать важную информацию
- ❌ **Скользящее окно** - теряется весь начальный контекст
- ❌ **Случайная выборка** - разрушается связность повествования

**Выбрали:**
- ✅ **Гибридная суммаризация** - разбиваем на части + суммаризируем каждую часть
  - Сохраняет всю важную информацию
  - Уменьшает объем в ~3-5 раз
  - Масштабируется на любую длину

## 🔧 Компоненты системы

```
┌─────────────────────────────────────────┐
│          Telegram Bot (index.js)        │
│  • Управление сессиями                  │
│  • Команды: /start, /finish, /cancel    │
│  • Накопление сообщений                 │
└────────────┬────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────┐
│     TokenCounter (utils/)               │
│  • Оценка количества токенов            │
│  • Проверка лимитов                     │
│  • Форматирование статистики            │
│  • Расчет стоимости                     │
└────────────┬────────────────────────────┘
             │
             ▼
      ┌──────┴──────┐
      │             │
      ▼             ▼
┌──────────┐  ┌───────────────────┐
│  Короткий│  │ Длинный (>6000)   │
│  текст   │  │                   │
└────┬─────┘  └───────┬───────────┘
     │                │
     │                ▼
     │        ┌─────────────────────┐
     │        │ TextSummarizer      │
     │        │  • Разбивка на части│
     │        │  • Суммаризация     │
     │        │  • Объединение      │
     │        └───────┬─────────────┘
     │                │
     └────────┬───────┘
              │
              ▼
┌──────────────────────────────────┐
│  CharacterAnalyzerAgent          │
│  • Анализ персонажей             │
│  • Психологические портреты      │
│  • Обнаружение отсутствия        │
└──────────────────────────────────┘
```

## 🔄 Алгоритм суммаризации

### 1. Проверка необходимости

```javascript
if (токены < 6000) {
  // Отправляем как есть
  return оригинальный_текст;
}
```

### 2. Разбиение на части

```javascript
// Делим по предложениям, ~2000 токенов на часть
const части = разбить_по_предложениям(текст, 2000);
// Результат: [часть1, часть2, часть3, ...]
```

### 3. Суммаризация каждой части

```javascript
for (каждая часть) {
  summary = await yandexGPT.summarize(часть);
  // Сжатие: 2000 токенов → ~650 токенов (~3x)
  саммари.push(summary);
}
```

### 4. Объединение

```javascript
итоговый_текст = саммари.join('\n\n');
// Итог: 8000 токенов → ~2600 токенов
```

## 📊 Примеры обработки

### Короткий текст (200 токенов)

```
Ввод: "Маша встретила волка..."
         ↓
[TokenCounter] → 200 токенов
         ↓
[Пропуск суммаризации]
         ↓
[CharacterAnalyzer] → Анализ
         ↓
Результат: 2 персонажа
Время: ~1.5 сек
Токены: 200 + 150 = 350
```

### Длинный текст (8000 токенов)

```
Ввод: [большой рассказ]
         ↓
[TokenCounter] → 8000 токенов ⚠️
         ↓
[TextSummarizer]
  • Разбивка → 4 части по 2000 токенов
  • Суммаризация:
    - Часть 1: 2000 → 650 (2.5 сек)
    - Часть 2: 2000 → 680 (2.3 сек)
    - Часть 3: 2000 → 630 (2.4 сек)
    - Часть 4: 2000 → 640 (2.6 сек)
  • Объединение → 2600 токенов
         ↓
[CharacterAnalyzer] → Анализ (2.8 сек)
         ↓
Результат: 6 персонажей
Время: ~12.6 сек
Токены: 8000 (вход) + 2400 (суммаризация) + 150 (анализ) = 10550
```

### Очень длинный текст (20000 токенов)

```
Ввод: [целая книга]
         ↓
[TokenCounter] → 20000 токенов ⚠️⚠️
         ↓
[TextSummarizer]
  • Разбивка → 10 частей по 2000 токенов
  • Суммаризация: 10 запросов (~25 сек)
  • Сжатие: 20000 → ~6500 токенов (3x)
         ↓
[CharacterAnalyzer] → Анализ (3.5 сек)
         ↓
Результат: все персонажи найдены
Время: ~28.5 сек
Токены: ~32500 total
Стоимость: ~9.75₽
```

## 💡 Ключевые решения

### 1. Оценка токенов

```javascript
// Для русского языка:
токены ≈ длина_текста / 2.5
```

**Почему 2.5?**
- Кириллица кодируется менее эффективно
- BPE-токенизация для русского: ~2-3 символа на токен
- Среднее значение с запасом

### 2. Размер частей (2000 токенов)

**Почему не больше?**
- YandexGPT-lite лимит: 8000 токенов
- Системный промпт: ~200 токенов
- Ответ модели: ~1500 токенов
- Запас: ~300 токенов
- **Итого:** 2000 + 200 + 1500 = 3700 ✅

**Почему не меньше?**
- Меньше частей = меньше запросов = быстрее
- Больше контекста на часть = лучше суммаризация

### 3. Порог суммаризации (6000 токенов)

**Почему 6000?**
- Системный промпт анализатора: ~400 токенов
- Текст: 6000 токенов
- Ответ: ~1500 токенов
- **Итого:** 7900 токенов < 8000 ✅

### 4. Температура моделей

```javascript
// Суммаризация
temperature: 0.3  // Низкая - точность важнее
// Анализ персонажей
temperature: 0.5  // Средняя - баланс точности и креативности
```

## 📈 Метрики и статистика

### Собираемые данные

```javascript
{
  // Входные данные
  originalTokens: 8000,
  originalLength: 20000,
  messageCount: 5,
  
  // Суммаризация (если была)
  summarization: {
    chunks: [
      { originalTokens: 2000, summaryTokens: 650, time: 2.5 },
      { originalTokens: 2000, summaryTokens: 680, time: 2.3 },
      // ...
    ],
    totalTokens: 2600,
    compressionRatio: 0.325, // 32.5% от оригинала
    totalTime: 10.2,
  },
  
  // Анализ
  analysis: {
    tokens: 350,
    time: 2.8,
    charactersFound: true,
  },
  
  // Итого
  totalTokens: 10550,
  totalTime: 13.0,
  estimatedCost: 3.165, // рубли
}
```

### Формат вывода

```
📊 Статистика входного текста:
━━━━━━━━━━━━━━━━━━━━━━━━━━
📝 Сообщений: 5
📏 Символов: 20,000
📊 Токенов (оценка): 8,000
━━━━━━━━━━━━━━━━━━━━━━━━━━

✅ Суммаризация завершена!
━━━━━━━━━━━━━━━━━━━━━━━━━━
✂️  Частей текста: 4
📉 Сжатие: 8,000 → 2,600 токенов
📊 Коэффициент: 32.5%
⏱️  Время: 10.20с
💰 Стоимость: ~3.15₽
━━━━━━━━━━━━━━━━━━━━━━━━━━
```

## 🎓 Обучающие моменты

### 1. Управление контекстом

- **Проблема:** Лимит контекста модели
- **Решение:** Разбиение + суммаризация
- **Компромисс:** Больше запросов, больше времени

### 2. Баланс точности и производительности

- **Большие части:** Меньше запросов, но риск превысить лимит
- **Маленькие части:** Больше запросов, но безопаснее
- **Наш выбор:** 2000 токенов - золотая середина

### 3. Обработка edge cases

- Тексты без персонажей → специальная обработка
- Очень короткие тексты → пропуск суммаризации
- Одно длинное предложение → разбиение по словам

### 4. UX при долгих операциях

- Информирование о процессе
- Промежуточные статусы
- Детальная статистика после завершения

## 🔮 Возможные улучшения

1. **Кэширование** - сохранение суммаризированных текстов
2. **Параллелизация** - суммаризация частей параллельно (требует rate limits)
3. **Адаптивный размер частей** - динамическая подстройка под контент
4. **Точный подсчет токенов** - использование официального токенизатора
5. **Streaming** - постепенный вывод результатов
6. **База данных** - персистентность сессий

## 📚 Используемые технологии

- **Telegraf** - Telegram Bot API
- **YandexGPT-lite** - LLM для суммаризации и анализа
- **Node.js** - Runtime
- **ES Modules** - Современный JavaScript

## 🎯 Достижение целей

✅ **Подсчет токенов** - реализован через TokenCounter  
✅ **Сравнение длины** - примеры в test-examples.md  
✅ **Обработка превышения** - автоматическая суммаризация  
✅ **Детальная статистика** - токены, время, стоимость на каждом этапе  
✅ **Масштабируемость** - работает с текстами любой длины  

